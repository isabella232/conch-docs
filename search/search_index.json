{
    "docs": [
        {
            "location": "/",
            "text": "This documentation is very much a work in progress.\n\n\n\n\nConch helps you build and manage datacenters.\n\n\nConch's goal is to provide an end-to-end solution for full datacenter resource\nlifecycle: from design to initial power-on to end-of-life for all components of\nall devices.\n\n\nConch is open source, licensed under MPL2.",
            "title": "Home"
        },
        {
            "location": "/about/",
            "text": "",
            "title": "Overview"
        },
        {
            "location": "/architecture/",
            "text": "Components\n\u00b6\n\n\nAPI\n\u00b6\n\n\nConch's core is its REST API. The API is documented \nhere\n.\n\n\nIt exposes basic CRUD for all resources we know how to manage:\n\n\n\n\nUsers\n\n\nWorkspaces\n\n\nDatacenters, rooms (soon: cages)\n\n\nRacks\n\n\nHardware products (servers, switches)\n\n\nDevices\n\n\nValidation failures\n\n\nStats\n\n\n\n\nWorkspaces are arbitrary collections of Datacenter Rooms or Racks. This is\nuseful for a number of reasons: You can define workspaces for AZs, for\nexpansions, or for specific builds. You can invite specific users to a given\nworkspace, allowing you to limit the devices an outside vendor can interact\nwith. Workspaces are a very powerful, useful primitive.\n\n\nIt also includes report ingestion and validation endpoints. These feed into the\n\nvalidation engine\n\nwhich allows us to decide if a device is healthy or not, based off its hardware\nprofile, environmental or arbitrary data.\n\n\nWriting and testing new validations is documented\n\nhere\n.\n\n\nThe APIs are written in Perl's \nMojolicious framework\n, and are available\n\nhere\n.\n\n\nA basic workspace-aware stats framework is available \nhere\n.\n\n\nUI\n\u00b6\n\n\nThe Conch UI is rapidly evolving. Its initial design was targeted at hardware\nintegrators and datacenter operation staff -- the main focus was on defining\nrack layout and identifying problems with devices in those racks.\n\n\nAs time goes on, the UI will expand to include better search and reporting\noptions, and more advanced features like datacenter, rack, and BOM design.\n\n\nThe UI is an API consumer, and is not magical in any respect.\n\n\nThe UI is written in \nMithril.js\n, and is available\n\nhere\n.\n\n\nConch Shell\n\u00b6\n\n\nThe Conch Shell is a CLI tool provides many useful primitives for interacting\nwith the Conch API. It supports multiple user profiles and endpoints, and has\nJSON output options to allow users to create arbitrary processes with it.\n\n\nThe Shell has many options. Here are some examples of using it:\n\n\n\n\nOverview\n\n\nRack slot contents\n\n\nValidation plans\n\n\nHardware profiles\n\n\n\n\nThe Shell is an API consumer, and is not magical in any respect.\n\n\nThe Shell is written on Go, and is available \nhere\n.\n\n\nDatabase\n\u00b6\n\n\nConch's core database is Postgres.\n\n\nConch uses a \nsimple migration system\n\nfor managing database changes.\n\n\nRelay\n\u00b6\n\n\nConch Relay is a simple API service that takes traffic from the livesys or other\nConch clients and interacts with the Conch API. It is currently used mainly in\nintegration and initial validation stages of datacenter builds.\n\n\nThe Relay codebase is currently closed, but is planned on being open ASAP.\n\n\nThe Relay comes in two flavors:\n\n\nDiagnostic Relay Device (DRD)\n\u00b6\n\n\nAKA Preflight Relay Device (PRD).\n\n\nThis is a physical deployment of the Relay service. DRDs are simple x86 or\nRasberry Pi devices that run the Relay and various other agents for standing up\nand configuring racks.\n\n\nIn this mode, we support configuration of TORs. As such, the device is plugged\ninto individual racks. In certain configurations, the DRD must have serial\ncables plugged into the TORs to configure them. Other switches only require\nethernet access to do so.\n\n\nThis mode is exclusing used in off-site integration facilities, before the racks\nare shipped to the datacenter.\n\n\nThe Relay also includes a support tunnel feature, so engineers can remotely log\ninto the integration facility if needed.\n\n\nRelay Service (VM)\n\u00b6\n\n\nIn this mode, the Relay runs in a SmartOS or VM, and operates in a post-shipment\nre-validation mode. In the future, this mode may also allow the planned\nproduction inventory agent to submit reports to the Conch APIs from a local\nservice.\n\n\nLivesys\n\u00b6\n\n\nThe live system is a read-only Linux image the Relay PXE boots on servers.\n\n\nThe livesys includes a number of agents:\n\n\n\n\nFirmware upgrade\n\n\nReporter\n\n\nRebooter\n\n\nBurnin\n\n\n...\n\n\n\n\nThe livesys is configured via \nchef-solo\n cookbooks.\n\n\nThe livesys codebase is currently closed, but is planned on being open ASAP.",
            "title": "Architecture"
        },
        {
            "location": "/architecture/#components",
            "text": "",
            "title": "Components"
        },
        {
            "location": "/architecture/#api",
            "text": "Conch's core is its REST API. The API is documented  here .  It exposes basic CRUD for all resources we know how to manage:   Users  Workspaces  Datacenters, rooms (soon: cages)  Racks  Hardware products (servers, switches)  Devices  Validation failures  Stats   Workspaces are arbitrary collections of Datacenter Rooms or Racks. This is\nuseful for a number of reasons: You can define workspaces for AZs, for\nexpansions, or for specific builds. You can invite specific users to a given\nworkspace, allowing you to limit the devices an outside vendor can interact\nwith. Workspaces are a very powerful, useful primitive.  It also includes report ingestion and validation endpoints. These feed into the validation engine \nwhich allows us to decide if a device is healthy or not, based off its hardware\nprofile, environmental or arbitrary data.  Writing and testing new validations is documented here .  The APIs are written in Perl's  Mojolicious framework , and are available here .  A basic workspace-aware stats framework is available  here .",
            "title": "API"
        },
        {
            "location": "/architecture/#ui",
            "text": "The Conch UI is rapidly evolving. Its initial design was targeted at hardware\nintegrators and datacenter operation staff -- the main focus was on defining\nrack layout and identifying problems with devices in those racks.  As time goes on, the UI will expand to include better search and reporting\noptions, and more advanced features like datacenter, rack, and BOM design.  The UI is an API consumer, and is not magical in any respect.  The UI is written in  Mithril.js , and is available here .",
            "title": "UI"
        },
        {
            "location": "/architecture/#conch-shell",
            "text": "The Conch Shell is a CLI tool provides many useful primitives for interacting\nwith the Conch API. It supports multiple user profiles and endpoints, and has\nJSON output options to allow users to create arbitrary processes with it.  The Shell has many options. Here are some examples of using it:   Overview  Rack slot contents  Validation plans  Hardware profiles   The Shell is an API consumer, and is not magical in any respect.  The Shell is written on Go, and is available  here .",
            "title": "Conch Shell"
        },
        {
            "location": "/architecture/#database",
            "text": "Conch's core database is Postgres.  Conch uses a  simple migration system \nfor managing database changes.",
            "title": "Database"
        },
        {
            "location": "/architecture/#relay",
            "text": "Conch Relay is a simple API service that takes traffic from the livesys or other\nConch clients and interacts with the Conch API. It is currently used mainly in\nintegration and initial validation stages of datacenter builds.  The Relay codebase is currently closed, but is planned on being open ASAP.  The Relay comes in two flavors:",
            "title": "Relay"
        },
        {
            "location": "/architecture/#diagnostic-relay-device-drd",
            "text": "AKA Preflight Relay Device (PRD).  This is a physical deployment of the Relay service. DRDs are simple x86 or\nRasberry Pi devices that run the Relay and various other agents for standing up\nand configuring racks.  In this mode, we support configuration of TORs. As such, the device is plugged\ninto individual racks. In certain configurations, the DRD must have serial\ncables plugged into the TORs to configure them. Other switches only require\nethernet access to do so.  This mode is exclusing used in off-site integration facilities, before the racks\nare shipped to the datacenter.  The Relay also includes a support tunnel feature, so engineers can remotely log\ninto the integration facility if needed.",
            "title": "Diagnostic Relay Device (DRD)"
        },
        {
            "location": "/architecture/#relay-service-vm",
            "text": "In this mode, the Relay runs in a SmartOS or VM, and operates in a post-shipment\nre-validation mode. In the future, this mode may also allow the planned\nproduction inventory agent to submit reports to the Conch APIs from a local\nservice.",
            "title": "Relay Service (VM)"
        },
        {
            "location": "/architecture/#livesys",
            "text": "The live system is a read-only Linux image the Relay PXE boots on servers.  The livesys includes a number of agents:   Firmware upgrade  Reporter  Rebooter  Burnin  ...   The livesys is configured via  chef-solo  cookbooks.  The livesys codebase is currently closed, but is planned on being open ASAP.",
            "title": "Livesys"
        },
        {
            "location": "/features/",
            "text": "Feature Checklist\n\u00b6\n\n\nArchitecture\n\u00b6\n\n\n\n\n Multi-tentant web service\n\n\n Basic user roles\n\n\n Rest APIs\n\n\n CLI tool\n\n\n Workspaces\n\n\n Validation engine\n\n\n User settings (KV)\n\n\n Device settings (KV)\n\n\n Feature flags\n\n\n Validation configuration\n\n\n Organizations\n\n\n Organization settings (KV)\n\n\n Korean localization (partial)\n\n\n\n\nDatacenter Design and Visualization\n\u00b6\n\n\n\n\n Basic hardware profile support\n\n\n Robust hardware profile support\n\n\n IPAM\n\n\n BOM designer\n\n\n Rack designer\n\n\n Datacenter designer\n\n\n Design review and approval\n\n\n\n\nAsset Management\n\u00b6\n\n\n\n\n Server tracking\n\n\n TOR tracking\n\n\n Component database\n\n\n Parts and supply tracking\n\n\n Preventative maintenance\n\n\n Maintenance schedules\n\n\n Work orders via JIRA integration or similar\n\n\n Spare management\n\n\n Build reports\n\n\n Failure reports\n\n\n Validation/audit reports\n\n\n\n\nProcurement and RFQs\n\u00b6\n\n\n\n\n Emit full BOMs from a datacenter workspace\n\n\n\n\nPreflight\n\u00b6\n\n\n\"Preflight\" is the initial stage of a device entering service. This may happen\nduring hardware integration, or during datacenter standup.\n\n\n\n\n Embedded Relay Devices for off-site usage (rack integration)\n\n\n Linux-based live system (PXE booted)\n\n\n Server firmware upgrade\n\n\n Server configuration\n\n\n Server validation\n\n\n Server burnin\n\n\n TOR firmware upgrade\n\n\n TOR configuration\n\n\n TOR basic validation\n\n\n TOR extended validation\n\n\n TOR burnin\n\n\n Server/TOR network map validation\n\n\n Network stress testing (intra-rack)\n\n\n Network stress testing (inter-rack)\n\n\n Network stress testing (cross-DC)\n\n\n Burnin/stress metrics stored in TSDB\n\n\n PDU firmware upgrade\n\n\n PDU configuration\n\n\n Server/PDU power map\n\n\n Multi-OS boot\n\n\n Multi-OS burnin\n\n\n\n\nServices Standup\n\u00b6\n\n\n\n\n Admin server\n\n\n Triton Headnode\n\n\n Triton Compute Node\n\n\n Manta initial install\n\n\n Manta storage expansion\n\n\n Manta metadata expansion\n\n\n\n\nDevice Production\n\u00b6\n\n\nProduction is the longest (hopefully!) stage of a device during its lifecycle.\n\n\n\n\n VM-based Relay software for on-site usage\n\n\n Agent-based version of the livesys reporter\n\n\n Diagnostics mode\n\n\n\n\nDevice Retirement\n\u00b6\n\n\n\n\n API and UI for marking devices retired",
            "title": "Features"
        },
        {
            "location": "/features/#feature-checklist",
            "text": "",
            "title": "Feature Checklist"
        },
        {
            "location": "/features/#architecture",
            "text": "Multi-tentant web service   Basic user roles   Rest APIs   CLI tool   Workspaces   Validation engine   User settings (KV)   Device settings (KV)   Feature flags   Validation configuration   Organizations   Organization settings (KV)   Korean localization (partial)",
            "title": "Architecture"
        },
        {
            "location": "/features/#datacenter-design-and-visualization",
            "text": "Basic hardware profile support   Robust hardware profile support   IPAM   BOM designer   Rack designer   Datacenter designer   Design review and approval",
            "title": "Datacenter Design and Visualization"
        },
        {
            "location": "/features/#asset-management",
            "text": "Server tracking   TOR tracking   Component database   Parts and supply tracking   Preventative maintenance   Maintenance schedules   Work orders via JIRA integration or similar   Spare management   Build reports   Failure reports   Validation/audit reports",
            "title": "Asset Management"
        },
        {
            "location": "/features/#procurement-and-rfqs",
            "text": "Emit full BOMs from a datacenter workspace",
            "title": "Procurement and RFQs"
        },
        {
            "location": "/features/#preflight",
            "text": "\"Preflight\" is the initial stage of a device entering service. This may happen\nduring hardware integration, or during datacenter standup.    Embedded Relay Devices for off-site usage (rack integration)   Linux-based live system (PXE booted)   Server firmware upgrade   Server configuration   Server validation   Server burnin   TOR firmware upgrade   TOR configuration   TOR basic validation   TOR extended validation   TOR burnin   Server/TOR network map validation   Network stress testing (intra-rack)   Network stress testing (inter-rack)   Network stress testing (cross-DC)   Burnin/stress metrics stored in TSDB   PDU firmware upgrade   PDU configuration   Server/PDU power map   Multi-OS boot   Multi-OS burnin",
            "title": "Preflight"
        },
        {
            "location": "/features/#services-standup",
            "text": "Admin server   Triton Headnode   Triton Compute Node   Manta initial install   Manta storage expansion   Manta metadata expansion",
            "title": "Services Standup"
        },
        {
            "location": "/features/#device-production",
            "text": "Production is the longest (hopefully!) stage of a device during its lifecycle.    VM-based Relay software for on-site usage   Agent-based version of the livesys reporter   Diagnostics mode",
            "title": "Device Production"
        },
        {
            "location": "/features/#device-retirement",
            "text": "API and UI for marking devices retired",
            "title": "Device Retirement"
        },
        {
            "location": "/roadmap/",
            "text": "Roadmap\n\u00b6\n\n\n2018H1 Goals\n\u00b6\n\n\n\n\nTriton CN setup automation\n\n\nNetwork stress v1\n\n\nArista/Cisco TOR support\n\n\nAudit report generation\n\n\nProduction inventory agent\n\n\nDatacenter designer\n\n\n\n\n2018H2 Goals\n\u00b6\n\n\n\n\nManta expansion automation\n\n\nSwitch VLAN API\n\n\nAdmin server install\n\n\nStore burnin in TSDB\n\n\nReporting\n\n\nTriton testing v2\n\n\nManta testing v2\n\n\nDiagnostics mode\n\n\nMulti-OS boot / burnin\n\n\nBOM builder",
            "title": "Roadmap"
        },
        {
            "location": "/roadmap/#roadmap",
            "text": "",
            "title": "Roadmap"
        },
        {
            "location": "/roadmap/#2018h1-goals",
            "text": "Triton CN setup automation  Network stress v1  Arista/Cisco TOR support  Audit report generation  Production inventory agent  Datacenter designer",
            "title": "2018H1 Goals"
        },
        {
            "location": "/roadmap/#2018h2-goals",
            "text": "Manta expansion automation  Switch VLAN API  Admin server install  Store burnin in TSDB  Reporting  Triton testing v2  Manta testing v2  Diagnostics mode  Multi-OS boot / burnin  BOM builder",
            "title": "2018H2 Goals"
        },
        {
            "location": "/usage/",
            "text": "Usage\n\u00b6\n\n\nConch is meant to enable datacenter operators and integrators to effectively\nmanage and build datacenters.\n\n\nWe provide a reactive UI written in mithril.js for the web interface, and a\ncommand-line utility written in Go.\n\n\nBoth the web UI and CLI use the same Conch API -- but they may expose different\nfeatures. Generally speaking, the web UI is useful for Datacenter Operations and\nIntegration staff, and the CLI tool is useful for System Administrators or\noperating Conch itself.",
            "title": "Overview"
        },
        {
            "location": "/usage/#usage",
            "text": "Conch is meant to enable datacenter operators and integrators to effectively\nmanage and build datacenters.  We provide a reactive UI written in mithril.js for the web interface, and a\ncommand-line utility written in Go.  Both the web UI and CLI use the same Conch API -- but they may expose different\nfeatures. Generally speaking, the web UI is useful for Datacenter Operations and\nIntegration staff, and the CLI tool is useful for System Administrators or\noperating Conch itself.",
            "title": "Usage"
        },
        {
            "location": "/usage/ui/",
            "text": "",
            "title": "Web UI"
        },
        {
            "location": "/usage/cli/",
            "text": "",
            "title": "CLI"
        },
        {
            "location": "/relay/",
            "text": "Diagnostic Relay Device (DRD)\n\u00b6\n\n\nConch Diagnostic Relay Devices are small x86-based devices which are attached to\npre-production racks to ensure that the servers, switches, and network cabling\ninside an assembled rack conform to Joyent\u2019s specifications regarding correct\nnetwork wiring harness, firmware versions, and hardware component manifests.\n\n\nDRDs\n are generally used in off-site integration by third-party vendors.\n\n\n\n\nNote\n\n\nThe DRD was previously known as the Preflight Relay Device.\n\n\n\n\nOnce all devices are verified to be correct, the DRD performs burn-in testing of\nthe CPU, DRAM, and disks of each system and then powers the system off when\ncompleted.\n\n\nJoyent staff track the progress of the systems undergoing preflight testing live\nand advise integration staff on any anomalies which require correcting. The goal\nis to verify correct configurations and to root out failing or DOA components\nprior to their departure to the datacenter.\n\n\n\n\nAutomated Upgrades and Validation\n\u00b6\n\n\nEach switch and server come from the manufacturer with the configuration\nspecified in Bill of Materials, but it has not yet been customized for\nproduction.\n\n\nThe DRD Preflight environment configures and upgrades the host to Joyent\u2019s\noperating standards, using automation with scripts and software the DRDs upgrade\nand customize each host.\n\n\nThis includes the following settings:\n\n\n\n\nTop of rack switch firmware\n\n\nServer firmware for BIOS, HBA, backplane and hard drives\n\n\nCustom boot and environmental settings.\n\n\n\n\nUpgrades will automatically reboot switches and servers when necessary.\n\n\n\n\nWarning\n\n\nIt is important to maintain power to the rack during this entire process.\n\n\n\n\nIn the event that power is shut off to the rack, it should be recorded and\nreported to Joyent Build Operations.\n\n\nDuring the upgrade process the switches and servers are continuously checked for\nalignment with the production specifications.\n\n\nThis process can be viewed at \nhttps://conch.joyent.us\n using the supplied\ncredentials.\n\n\nHosts that are reporting into the interface have the cloud icon with the arrow\npointing up and if they are not actively reporting a question mark will appear.\nAs settings are validated a checkmark will appear. If there are errors an\nexclamation point will show.\n\n\nFinally, when a device reaches all of the stages of upgrading and validation a\ncheck mark with a solid colored circle will appear and the \u201cPASS\" box will show\nfor the device.",
            "title": "Overview"
        },
        {
            "location": "/relay/#diagnostic-relay-device-drd",
            "text": "Conch Diagnostic Relay Devices are small x86-based devices which are attached to\npre-production racks to ensure that the servers, switches, and network cabling\ninside an assembled rack conform to Joyent\u2019s specifications regarding correct\nnetwork wiring harness, firmware versions, and hardware component manifests.  DRDs  are generally used in off-site integration by third-party vendors.   Note  The DRD was previously known as the Preflight Relay Device.   Once all devices are verified to be correct, the DRD performs burn-in testing of\nthe CPU, DRAM, and disks of each system and then powers the system off when\ncompleted.  Joyent staff track the progress of the systems undergoing preflight testing live\nand advise integration staff on any anomalies which require correcting. The goal\nis to verify correct configurations and to root out failing or DOA components\nprior to their departure to the datacenter.",
            "title": "Diagnostic Relay Device (DRD)"
        },
        {
            "location": "/relay/#automated-upgrades-and-validation",
            "text": "Each switch and server come from the manufacturer with the configuration\nspecified in Bill of Materials, but it has not yet been customized for\nproduction.  The DRD Preflight environment configures and upgrades the host to Joyent\u2019s\noperating standards, using automation with scripts and software the DRDs upgrade\nand customize each host.  This includes the following settings:   Top of rack switch firmware  Server firmware for BIOS, HBA, backplane and hard drives  Custom boot and environmental settings.   Upgrades will automatically reboot switches and servers when necessary.   Warning  It is important to maintain power to the rack during this entire process.   In the event that power is shut off to the rack, it should be recorded and\nreported to Joyent Build Operations.  During the upgrade process the switches and servers are continuously checked for\nalignment with the production specifications.  This process can be viewed at  https://conch.joyent.us  using the supplied\ncredentials.  Hosts that are reporting into the interface have the cloud icon with the arrow\npointing up and if they are not actively reporting a question mark will appear.\nAs settings are validated a checkmark will appear. If there are errors an\nexclamation point will show.  Finally, when a device reaches all of the stages of upgrading and validation a\ncheck mark with a solid colored circle will appear and the \u201cPASS\" box will show\nfor the device.",
            "title": "Automated Upgrades and Validation"
        },
        {
            "location": "/relay/setup/",
            "text": "Initial Setup of Diagnostic Relay Devices (DRD)\n\u00b6\n\n\nBill of Materials\n\u00b6\n\n\nUpon receiving the Diagnostic Relay Device equipment, the shipment should be\ninspected for obvious damage or tampering.\n\n\nAny suspected damage should immediately be reported to Joyent Build Operations,\nwho can be reached at \npreflight-dev@joyent.com\n\n\nAll hardware is engineered and assembled to be robust and free of maintenance\ntasks once the DRDs are deployed. \n\n\nThe DRD shipment consists of:\n\n\n\n\n802.11n/ac wireless access point\n\n\nDiagnostic Relay Devices (DRD) with wifi antenna kits\n\n\nIEC C13-C14 power cables (blue) and AC-DC power supplies\n\n\n1gig copper SFPs\n\n\n2 CAT5 ethernet cables per DRD\n\n\n\n\nDevice Assembly\n\u00b6\n\n\nAfter unpacking, initial setup of the DRD system consists of the following\nsteps:\n\n\nUnbox the DRDs and attach the two wifi antennas included in each box to the\ngold coaxial connectors on rear of the DRD, ensuring that the antennas are\nscrewed on securely.\n\n\nUnpackage the IEC C13-C14 power cables and AC/DC power supplies and match a set\nof these to each DRD.\n\n\nWireless Connectivity\n\u00b6\n\n\nLocate an ethernet jack for the premises\u2019 network and a AC power outlet\nwhich are in reasonable wifi range of the area where the DRDs will be\noperating.\n\n\nPlug the provided wireless access point into these. This access point is fully\nsecured and acts as the bridge for the DRDs to the outside world, and thus to\nJoyent\u2019s Conch servers. It broadcasts a wireless network with the SSID of\n\u201cpreflight\u201d.\n\n\nOnly the DRDs will be able to access this SSID in order to send and receive\ntelemetry from Joyent. After initializing, the access point should display a\nlarge blue light on its top indicating that it is operating correctly and was\nable to DHCP an IP address from the local network.\n\n\nThe DRD Wireless Access Point requires access to the public internet on the\nfollowing TCP+UDP ports:\n\n\n\n\nTCP 22 (SSH)\n\n\nUDP+TCP 53 (DNS)\n\n\nUDP 123 (NTP)\n\n\nTCP 443 (HTTPS)\n\n\n\n\nAt this point, the DRD system setup is complete and, if powered on, the DRDs\nwill ping home to the Joyent Conch servers. They are now ready to be employed.\nA small blue light on the rear of each DRD indicates that it is powered on.\nThey have no on/off switch, only a small, recessed reset button in the rear\nco-located with the blue status lamp next to the DC power input.",
            "title": "Setup"
        },
        {
            "location": "/relay/setup/#initial-setup-of-diagnostic-relay-devices-drd",
            "text": "",
            "title": "Initial Setup of Diagnostic Relay Devices (DRD)"
        },
        {
            "location": "/relay/setup/#bill-of-materials",
            "text": "Upon receiving the Diagnostic Relay Device equipment, the shipment should be\ninspected for obvious damage or tampering.  Any suspected damage should immediately be reported to Joyent Build Operations,\nwho can be reached at  preflight-dev@joyent.com  All hardware is engineered and assembled to be robust and free of maintenance\ntasks once the DRDs are deployed.   The DRD shipment consists of:   802.11n/ac wireless access point  Diagnostic Relay Devices (DRD) with wifi antenna kits  IEC C13-C14 power cables (blue) and AC-DC power supplies  1gig copper SFPs  2 CAT5 ethernet cables per DRD",
            "title": "Bill of Materials"
        },
        {
            "location": "/relay/setup/#device-assembly",
            "text": "After unpacking, initial setup of the DRD system consists of the following\nsteps:  Unbox the DRDs and attach the two wifi antennas included in each box to the\ngold coaxial connectors on rear of the DRD, ensuring that the antennas are\nscrewed on securely.  Unpackage the IEC C13-C14 power cables and AC/DC power supplies and match a set\nof these to each DRD.",
            "title": "Device Assembly"
        },
        {
            "location": "/relay/setup/#wireless-connectivity",
            "text": "Locate an ethernet jack for the premises\u2019 network and a AC power outlet\nwhich are in reasonable wifi range of the area where the DRDs will be\noperating.  Plug the provided wireless access point into these. This access point is fully\nsecured and acts as the bridge for the DRDs to the outside world, and thus to\nJoyent\u2019s Conch servers. It broadcasts a wireless network with the SSID of\n\u201cpreflight\u201d.  Only the DRDs will be able to access this SSID in order to send and receive\ntelemetry from Joyent. After initializing, the access point should display a\nlarge blue light on its top indicating that it is operating correctly and was\nable to DHCP an IP address from the local network.  The DRD Wireless Access Point requires access to the public internet on the\nfollowing TCP+UDP ports:   TCP 22 (SSH)  UDP+TCP 53 (DNS)  UDP 123 (NTP)  TCP 443 (HTTPS)   At this point, the DRD system setup is complete and, if powered on, the DRDs\nwill ping home to the Joyent Conch servers. They are now ready to be employed.\nA small blue light on the rear of each DRD indicates that it is powered on.\nThey have no on/off switch, only a small, recessed reset button in the rear\nco-located with the blue status lamp next to the DC power input.",
            "title": "Wireless Connectivity"
        },
        {
            "location": "/relay/usage/",
            "text": "Usage and Validation Procedures\n\u00b6\n\n\nWhen it comes time to validate an assembled rack, Joyent Build Operations will\ndirect integration personnel to attach a DRD to a rack in the manner prescribed\nbelow. At this point, all devices in the rack should be completely powered off.\n\n\nSwitches \nMust\n be Factory Default\n\u00b6\n\n\nIf any previous burn-in was done on the rack, all switches in the rack must be\nreset to their factory defaults. Any non-default configuration will confuse the\nautomation and cause preflight to fail.\n\n\nAttaching The DRD\n\u00b6\n\n\nA DRD requires 2 ethernet and 1 power connection to a rack:\n\n\n\n\nETH0 should be plugged into port 21 on the bottom Arista switch\n\n\nETH1 should be plugged into port 21 on Cisco management switch\n\n\n\n\nThe cable for ETH1/Arista requires the included 1gig copper SFP module.\n\n\n\n\nThe AC-DC power adaptor. Ensure that the DRD is powered and the blue lamp on the\nrear of the unit is illuminated. The blue lamp is a LED that is visible through\nthe reset button access port to the upper-right of the DC power connector.\n\n\nPowering On The Racks\n\u00b6\n\n\nBefore applying power to the rack, all serial numbers need to be entered into\nthe Rack Layout. The contents of each rack are predefined and do not require\nmodification.\n\n\nThe only data entry required is entering the serial numbers of the devices in\nthe rack.\n\n\nFind the specific rack you are working on in the Browse section of the Conch Web\nUI. Click the orange \"edit assignments\" button.\n\n\n\n\nEnter the serial numbers for the systems in each Rack Unit, and then click Save.\n\n\nIf asset tags are being applied to the system, they should also be entered at\nthis time.\n\n\n\n\nWhen all serial numbers are entered into the text boxes associated with their\nrack unit, press the Assign Devices button to save the information.\n\n\nThe rack is now ready to be powered on.  When the rack\u2019s PDUs are switched on,\nthe switches, DRD and server BMCs should power on. From this point, the process\nis hands-off.\n\n\nThe DRD is programmed to start the servers after assigning IP addresses to the\nserver BMCs.\n\n\nThe switches and servers will immediately enter the automated upgrade and\nvalidation process.\n\n\nCompleting A Rack\n\u00b6\n\n\nWhen the DRD has finished validating a server and all tests are successfully\npassed, the server will go through a period of burn-in lasting several hours. A\ntimer will display how long a given device has left to burn-in when viewing\nDevice Details.\n\n\nUpon completion and if no components fail during the burn-in process, the server\nwill be automatically powered down. When all servers in a rack have reached this\nstage, the rack is considered complete, and the DRD may be removed to the next\nrack to be put through the Preflight process at the direction of Joyent Build\nOperations.",
            "title": "Usage"
        },
        {
            "location": "/relay/usage/#usage-and-validation-procedures",
            "text": "When it comes time to validate an assembled rack, Joyent Build Operations will\ndirect integration personnel to attach a DRD to a rack in the manner prescribed\nbelow. At this point, all devices in the rack should be completely powered off.",
            "title": "Usage and Validation Procedures"
        },
        {
            "location": "/relay/usage/#switches-must-be-factory-default",
            "text": "If any previous burn-in was done on the rack, all switches in the rack must be\nreset to their factory defaults. Any non-default configuration will confuse the\nautomation and cause preflight to fail.",
            "title": "Switches Must be Factory Default"
        },
        {
            "location": "/relay/usage/#attaching-the-drd",
            "text": "A DRD requires 2 ethernet and 1 power connection to a rack:   ETH0 should be plugged into port 21 on the bottom Arista switch  ETH1 should be plugged into port 21 on Cisco management switch   The cable for ETH1/Arista requires the included 1gig copper SFP module.   The AC-DC power adaptor. Ensure that the DRD is powered and the blue lamp on the\nrear of the unit is illuminated. The blue lamp is a LED that is visible through\nthe reset button access port to the upper-right of the DC power connector.",
            "title": "Attaching The DRD"
        },
        {
            "location": "/relay/usage/#powering-on-the-racks",
            "text": "Before applying power to the rack, all serial numbers need to be entered into\nthe Rack Layout. The contents of each rack are predefined and do not require\nmodification.  The only data entry required is entering the serial numbers of the devices in\nthe rack.  Find the specific rack you are working on in the Browse section of the Conch Web\nUI. Click the orange \"edit assignments\" button.   Enter the serial numbers for the systems in each Rack Unit, and then click Save.  If asset tags are being applied to the system, they should also be entered at\nthis time.   When all serial numbers are entered into the text boxes associated with their\nrack unit, press the Assign Devices button to save the information.  The rack is now ready to be powered on.  When the rack\u2019s PDUs are switched on,\nthe switches, DRD and server BMCs should power on. From this point, the process\nis hands-off.  The DRD is programmed to start the servers after assigning IP addresses to the\nserver BMCs.  The switches and servers will immediately enter the automated upgrade and\nvalidation process.",
            "title": "Powering On The Racks"
        },
        {
            "location": "/relay/usage/#completing-a-rack",
            "text": "When the DRD has finished validating a server and all tests are successfully\npassed, the server will go through a period of burn-in lasting several hours. A\ntimer will display how long a given device has left to burn-in when viewing\nDevice Details.  Upon completion and if no components fail during the burn-in process, the server\nwill be automatically powered down. When all servers in a rack have reached this\nstage, the rack is considered complete, and the DRD may be removed to the next\nrack to be put through the Preflight process at the direction of Joyent Build\nOperations.",
            "title": "Completing A Rack"
        },
        {
            "location": "/relay/status/",
            "text": "Status Information\n\u00b6\n\n\nStatus Information\n\u00b6\n\n\nStatus Page: Validation Progress\n\u00b6\n\n\nThe Status page contains a progress diagram. Each cell represents a rack.\n\n\nAs systems come online and begin validation, the cells in the bar will fill.\n\n\n\n\n\n\n\n\nColor\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGrey\n\n\nPending\n\n\n\n\n\n\nGreen\n\n\nAll devices validated\n\n\n\n\n\n\nBlue\n\n\nDevices in progress\n\n\n\n\n\n\nRed\n\n\nOne or more devices have failed validation.\n\n\n\n\n\n\n\n\n\n\nStatus Page: Validation Failures List\n\u00b6\n\n\nThe Status Page contains a summary of the devices with validation problems.\nClick View Device to open details and review the validation failures.\n\n\n\n\nDevice Details: View Validation Failure\n\u00b6\n\n\nWhen a device fails validation, the Validation tab in the Device Details view\nwill provide information on what has failed. In this case, the device's\nnetworking is not cabled correctly.\n\n\n\n\nDevice Details: Validation Complete\n\u00b6\n\n\nWhen a device has validated completely, the Device View will look something like\nthis:\n\n\n\n\nIcons Legend\n\u00b6\n\n\n\n\n\n\n\n\nIcon\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\nItem requires attention\n\n\n\n\n\n\n\n\nIf device, it is failing validation. If datacenter or rack listing, a device contained within is failing validation.\n\n\n\n\n\n\n\n\nDatacenter, rack, or device is currently being validated.\n\n\n\n\n\n\n\n\nNo report has been collected from this device yet.\n\n\n\n\n\n\n\n\nDevice has been validated and has been shut down.",
            "title": "Status and Icons"
        },
        {
            "location": "/relay/status/#status-information",
            "text": "",
            "title": "Status Information"
        },
        {
            "location": "/relay/status/#status-information_1",
            "text": "",
            "title": "Status Information"
        },
        {
            "location": "/relay/status/#status-page-validation-progress",
            "text": "The Status page contains a progress diagram. Each cell represents a rack.  As systems come online and begin validation, the cells in the bar will fill.     Color  Description      Grey  Pending    Green  All devices validated    Blue  Devices in progress    Red  One or more devices have failed validation.",
            "title": "Status Page: Validation Progress"
        },
        {
            "location": "/relay/status/#status-page-validation-failures-list",
            "text": "The Status Page contains a summary of the devices with validation problems.\nClick View Device to open details and review the validation failures.",
            "title": "Status Page: Validation Failures List"
        },
        {
            "location": "/relay/status/#device-details-view-validation-failure",
            "text": "When a device fails validation, the Validation tab in the Device Details view\nwill provide information on what has failed. In this case, the device's\nnetworking is not cabled correctly.",
            "title": "Device Details: View Validation Failure"
        },
        {
            "location": "/relay/status/#device-details-validation-complete",
            "text": "When a device has validated completely, the Device View will look something like\nthis:",
            "title": "Device Details: Validation Complete"
        },
        {
            "location": "/relay/status/#icons-legend",
            "text": "Icon  Description       Item requires attention     If device, it is failing validation. If datacenter or rack listing, a device contained within is failing validation.     Datacenter, rack, or device is currently being validated.     No report has been collected from this device yet.     Device has been validated and has been shut down.",
            "title": "Icons Legend"
        },
        {
            "location": "/development/",
            "text": "Development\n\u00b6\n\n\nDesign\n\u00b6\n\n\nMajor Conch features must be written as Request For Discussion documents (RFD).\n\n\nRFDs are published in the \nJoyent RFD GitHub repo\n.\n\n\n\n\n\n\n\n\nState\n\n\nRFD\n\n\n\n\n\n\n\n\n\n\nDeployed\n\n\nRFD 132 Conch: Unified Rack Integration Process\n\n\n\n\n\n\nDeployed\n\n\nRFD 133 Conch: Improved Device Validation\n\n\n\n\n\n\nDeployed\n\n\nRFD 134 Conch: User Access Control\n\n\n\n\n\n\nDraft\n\n\nRFD 135 Conch: Job Queue and Real-Time Notifications\n\n\n\n\n\n\nDraft\n\n\nRFD 136 Conch: Orchestration\n\n\n\n\n\n\nDraft\n\n\nRFD 140 Conch: Datacenter Designer\n\n\n\n\n\n\nPrivate\n\n\nTriton CN Setup Automation\n\n\n\n\n\n\n\n\nMinor features require discussion (often in GitHub Issues or email) but not\nRFDs.\n\n\nConch is designed in public as much as possible, though there some components\nthat are currently closed (until they can be scrubbed for security.)\n\n\nRepositories\n\u00b6\n\n\n\n\n\n\n\n\nRepo\n\n\nURL\n\n\n\n\n\n\n\n\n\n\nAPI\n\n\nhttps://github.com/joyent/conch\n\n\n\n\n\n\nShell\n\n\nhttps://github.com/joyent/conch-shell\n\n\n\n\n\n\nUI\n\n\nhttps://github.com/joyent/conch-ui\n\n\n\n\n\n\nStats API\n\n\nhttps://github.com/joyent/conch-stats\n\n\n\n\n\n\n\n\nOther code may be in private repositories and/or behind a VPN. Ask a member of\nthe Conch team for more info.\n\n\nProcess\n\u00b6\n\n\nAll software is developed using \ngit\n and the vast\nmajority of our code lives in \nGithub\n. As a rule,\nwe default to open source using the MPL2 license. There are situations where\ncode may be deemed too sensitive or too specific to a given client for the code\nto be fully open.  These are fairly rare situations, however, and our preference\nis to be as open as possible. \n\n\nIn most repositories, 'master' is a protected branch. Users cannot push to\nmaster directly and changes must be merged in via a pull request. Pull requests\nalmost universally require another developer to approve the changes. Most\nrepositories are also tied into\n\nBuildbot\n and must pass tests before a\nPR can be merged. This level of process is designed to help hold the team\naccountable for code changes and we strive for consensus in application designs\nand implementation.\n\n\nTesting\n\u00b6\n\n\nThe Conch API has an extensive test suite. As a general rule, incoming PRs must\nbe accompanied by tests and all tests must pass. Tests will be run by Buildbot\nwhenever a user pushes code up to Github (including topic and personal\nbranches) as well as when a PR is sent. We do not require topic and personal\nbranches to pass tests until they are ready for a PR.\n\n\nThe Conch Shell has a small but growing test suite. As with the API, tests must\npass for a PR. More importantly, being a Go application, incoming code must be\nformatted using gofmt and must pass gometalinter checks for best practices. See\nthe Makefile for the list of linters run during tests. Further, every test run\nattempts to build the application for all target platforms. PRs must pass tests,\nlinter checks, and must build on all target platforms.\n\n\nReleases\n\u00b6\n\n\nThe Conch API, UI, Stats, and Shell are all released using git tags, formatted\nin \nSemVer\n style. In the case of the API and Shell,\nreleases are managed via Buildbot. When a user pushes up a new tag, Buildbot\nexecutes the test suite. If tests pass, a new Github release is created, using\nthe tag's commit message as the body\n(\nexample\n).  \n\n\nFor the API and Shell, we've been building the tag body using a Ruby app named\n\ngithub-changelog-generator\n\nand copying in the data for just the current release.\n\n\nBuildbot\n\u00b6\n\n\nOur \nBuildbot\n setup is managed via\nAnsible. A Conch developer can provide a pointer to that repo. Currently,\nBuildbot executes all its work on a single FreeBSD KVM. To add a new repo to\nBuildbot, the master configuration must be updated and a Github webhook must be\nadded to the repository. The Conch API repo can be used as an example.\n\n\nSimilar Products\n\u00b6\n\n\n\n\nThe Foreman\n\n\nCollins\n\n\nnetbox\n\n\ndevice42\n\n\nCommercial CMMS (e.g., Fiix)\n\n\nGitHub Metal Cloud",
            "title": "Overview"
        },
        {
            "location": "/development/#development",
            "text": "",
            "title": "Development"
        },
        {
            "location": "/development/#design",
            "text": "Major Conch features must be written as Request For Discussion documents (RFD).  RFDs are published in the  Joyent RFD GitHub repo .     State  RFD      Deployed  RFD 132 Conch: Unified Rack Integration Process    Deployed  RFD 133 Conch: Improved Device Validation    Deployed  RFD 134 Conch: User Access Control    Draft  RFD 135 Conch: Job Queue and Real-Time Notifications    Draft  RFD 136 Conch: Orchestration    Draft  RFD 140 Conch: Datacenter Designer    Private  Triton CN Setup Automation     Minor features require discussion (often in GitHub Issues or email) but not\nRFDs.  Conch is designed in public as much as possible, though there some components\nthat are currently closed (until they can be scrubbed for security.)",
            "title": "Design"
        },
        {
            "location": "/development/#repositories",
            "text": "Repo  URL      API  https://github.com/joyent/conch    Shell  https://github.com/joyent/conch-shell    UI  https://github.com/joyent/conch-ui    Stats API  https://github.com/joyent/conch-stats     Other code may be in private repositories and/or behind a VPN. Ask a member of\nthe Conch team for more info.",
            "title": "Repositories"
        },
        {
            "location": "/development/#process",
            "text": "All software is developed using  git  and the vast\nmajority of our code lives in  Github . As a rule,\nwe default to open source using the MPL2 license. There are situations where\ncode may be deemed too sensitive or too specific to a given client for the code\nto be fully open.  These are fairly rare situations, however, and our preference\nis to be as open as possible.   In most repositories, 'master' is a protected branch. Users cannot push to\nmaster directly and changes must be merged in via a pull request. Pull requests\nalmost universally require another developer to approve the changes. Most\nrepositories are also tied into Buildbot  and must pass tests before a\nPR can be merged. This level of process is designed to help hold the team\naccountable for code changes and we strive for consensus in application designs\nand implementation.",
            "title": "Process"
        },
        {
            "location": "/development/#testing",
            "text": "The Conch API has an extensive test suite. As a general rule, incoming PRs must\nbe accompanied by tests and all tests must pass. Tests will be run by Buildbot\nwhenever a user pushes code up to Github (including topic and personal\nbranches) as well as when a PR is sent. We do not require topic and personal\nbranches to pass tests until they are ready for a PR.  The Conch Shell has a small but growing test suite. As with the API, tests must\npass for a PR. More importantly, being a Go application, incoming code must be\nformatted using gofmt and must pass gometalinter checks for best practices. See\nthe Makefile for the list of linters run during tests. Further, every test run\nattempts to build the application for all target platforms. PRs must pass tests,\nlinter checks, and must build on all target platforms.",
            "title": "Testing"
        },
        {
            "location": "/development/#releases",
            "text": "The Conch API, UI, Stats, and Shell are all released using git tags, formatted\nin  SemVer  style. In the case of the API and Shell,\nreleases are managed via Buildbot. When a user pushes up a new tag, Buildbot\nexecutes the test suite. If tests pass, a new Github release is created, using\nthe tag's commit message as the body\n( example ).    For the API and Shell, we've been building the tag body using a Ruby app named github-changelog-generator \nand copying in the data for just the current release.",
            "title": "Releases"
        },
        {
            "location": "/development/#buildbot",
            "text": "Our  Buildbot  setup is managed via\nAnsible. A Conch developer can provide a pointer to that repo. Currently,\nBuildbot executes all its work on a single FreeBSD KVM. To add a new repo to\nBuildbot, the master configuration must be updated and a Github webhook must be\nadded to the repository. The Conch API repo can be used as an example.",
            "title": "Buildbot"
        },
        {
            "location": "/development/#similar-products",
            "text": "The Foreman  Collins  netbox  device42  Commercial CMMS (e.g., Fiix)  GitHub Metal Cloud",
            "title": "Similar Products"
        },
        {
            "location": "/development/validations/",
            "text": "Writing, Deploying, and Testing a Conch Validation\n\u00b6\n\n\nOverview of steps\n\u00b6\n\n\n\n\nCreate a sub-class of \nConch::Validation\n, the Validation base class. Set\n   the \nname\n, \nversion\n, \ndescription\n, and \ncategory\n fields, and write a\n   \nvalidate\n method. The \nvalidate\n method body should register one or many\n   results or an error.\n\n\nCreate a test file in \nt/validation\n. Use the \nTest::Conch::Validation\n test\n   harness to write a test cases for your new validation. Verify it works as\n   expected, especially if there are edge cases in the logic.\n\n\nCommit the validation and validation test on a branch. \nCreate a pull\n   request in the Conch repository\n.\n   Request a review.\n\n\nOnce your pull request has been reviewed, merged into master, and deployed,\n   the validation is available on the system and via the API.\n\n\nUsing the \nConch Shell CLI tool\n, you\n   can do the following:\n\n\nList available validations. \nconch validations\n\n\nTest a validation against a device using supplied data. \nconch\n   validation VALIDATION_ID test DEVICE_ID\n\n\nAdd a validation to a validation plan. \nconch validation-plan\n   VALIDATION_PLAN add-validation VALIDATION_ID\n\n\n\n\n\n\n\n\nDocumentation for the \nValidation base class\n\nand \nValidation test harness\n\nare useful references for writing and testing a new Conch Validation.\n\n\nSetting up the Conch Repository\n\u00b6\n\n\nAll Conch Validations are code modules committed in the \nConch\nrepository\n. To add new validations, you must\ncreate a local clone of the repository and add new validations as files in the\nrepository.\n\n\nClone the repository locally and set it's path to your working directory. All\nfile paths in this tutorial are relative to the root of the repository.\n\n\nTo build and run Conch locally, Perl version 5.26.0,\n\nCarton\n, and\n\nyarn\n are all required to be installed on your system.\n\n\nTo build conch, run \nmake build\n in \nConch/\n. \nmake test\n runs all tests,\nincluding validation tests. To run the Conch server, run \nmake run\n in\n\nConch/\n. All valid Conch Validations are automatically loaded when the Conch\nserver is started.\n\n\nCreating a new Validation\n\u00b6\n\n\nLet's start by creating a simple yet valid validation. Create and write the\nfollowing code to the file \nConch/lib/Conch/Validation/MyValidation.pm\n:\n\n\npackage\n \nConch::Validation::MyValidation\n;\n\n\n\nuse\n \nMojo::Base\n \n'Conch::Validation'\n;\n\n\n\nhas\n \n'name'\n \n=>\n \n'my_validation_name'\n;\n\n\nhas\n \n'version'\n \n=>\n \n1\n;\n\n\nhas\n \n'category'\n \n=>\n \n'EXAMPLE'\n;\n\n\nhas\n \n'description'\n \n=>\n \nq(A basic example of a validation.)\n;\n\n\n\nsub\n \nvalidate\n \n{}\n\n\n\n1\n;\n\n\n\n\n\nThis validation doesn't do anything useful and will not record results when\nexecuted. But it satisfies all of the basic requirements to be loaded as a new\nValidation into Conch. You could run \nmake run\n in \nConch/\n and this validation\nwill be loaded into the system.\n\n\nValid validations must be sub-classes of the \nConch::Validation\n module, and\nthe package must be under the \nConch::Validation\n namespace. Validations use\nthe \nMojo::Base\n module for sub-classing\nand to set fields with the \nhas\n directive. The following fields are required\nfor every validation:\n\n\n\n\nname\n: a short string name of the validation. The \nname\n and \nversion\n\n  values together must be unique for all validations in the system.\n\n\nversion\n: an integer denoting the version of the validation for a given\n  name. This field allows for supporting multiple versions of similar\n  validations for legacy support.\n\n\ncategory\n: a short string signifier denoting the device component category\n  being validated. Example categories are \nBIOS\n, \nNET\n, \nDISK\n, \nCPU\n,\n  \nRAM\n. Upper case is used by convention.\n\n\ndescription\n: a longer, human-friendly description about the validation. The\n  description should describe what is being validated and how.\n\n\n\n\nAll validations must define the \nvalidate\n method. \nvalidate\n defines the logic\nof the validation. In our example, it does nothing, but next we will define\nvalidation logic and register results.\n\n\nWriting validation logic\n\u00b6\n\n\nThe \nvalidate\n method defines the validation logic. When a validation is run in\nthe validation system, the \nvalidate\n method is called for each validation.\n\n\nThe \nvalidate\n method receives two arguments: a 'self' reference and a hash ref\nof the data input to the validation. Like so:\n\n\nsub\n \nvalidate\n \n{\n\n    \nmy\n \n(\n$self\n,\n \n$data\n)\n \n=\n \n@_\n;\n\n    \n# assuming the input data has a 'product_name' field\n\n    \nmy\n \n$product_name\n \n=\n \n$data\n->\n{\nproduct_name\n};\n\n\n}\n\n\n\n\n\nThe validation logic should register at least one validation result and can\nregister as many results as desired. Validation results can be registered with\nthe following methods with different effects:\n\n\n\n\n\n\n$self->die('string')\n:\n  Stop execution of the validation and record a validation result with status\n  'error'. This should be used when the validation cannot continue. For example,\n  when expected values in the \n$data\n hash ref are not present, call\n  \n$self->die()\n with a description of the expected value\n\n\n\n\n\n\n$self->fail('string')\n:\n  Record a validation result with status 'fail' and continue execution of the\n  validation. This may be used if some precondition is not satisfied but the\n  validation should still continue.\n\n\n\n\n\n\n$self->register_result( expected => $a, got => $b, cmp => 'eq')\n:\n  this is the workhorse of validation logic. It takes an expected and 'got' value\n  and compares them with the operator \ncmp\n. The list of available comparison\n  operators can be \nfound in the documentation.\n\n\n\n\n\n\nFor our example, let's validate that the input data reports that the device has\nat least 1.21 gigawatts of power. This would be reported in JSON format and\nde-serialized to the Perl hash ref provided as the \n$data\n argument. The input\ndata should look like \n{ \"power\": { \"gigawatts\" : <number> } }\n. We must be\nsure to check we have received the data in the format we expect so our\nvalidation is verifying the expected values. Update your validation file to the\nfollowing:\n\n\npackage\n \nConch::Validation::MyValidation\n;\n\n\n\nuse\n \nMojo::Base\n \n'Conch::Validation'\n;\n\n\n\nhas\n \n'name'\n \n=>\n \n'my_validation_name'\n;\n\n\nhas\n \n'version'\n \n=>\n \n1\n;\n\n\nhas\n \n'category'\n \n=>\n \n'EXAMPLE'\n;\n\n\nhas\n \n'description'\n \n=>\n \nq(A basic example of a validation.)\n;\n\n\n\nsub\n \nvalidate\n \n{\n\n    \nmy\n \n(\n$self\n,\n \n$data\n)\n \n=\n \n@_\n;\n\n    \nmy\n \n$power\n \n=\n \n$data\n->\n{\npower\n};\n\n\n    \n$self\n->\ndie\n(\n\"'power' key is required in the input data\"\n)\n \nunless\n \ndefined\n(\n$power\n);\n\n    \n$self\n->\ndie\n(\n\"'power' value must be a hash\"\n)\n \nunless\n \nref\n(\n$power\n)\n \neq\n \n'HASH'\n;\n\n\n    \nmy\n \n$gigawatts\n \n=\n \n$power\n->\n{\ngigawatts\n};\n\n\n    \n$self\n->\ndie\n(\n\"'gigawatts' is required in the input data\"\n)\n \nunless\n \ndefined\n(\n$gigawatts\n);\n\n    \n$self\n->\ndie\n(\n\"\n'gigawatts'\n \nmust\n \nbe\n \na\n \nnumber\n') unless ( $gigawatts =~ /\\d+(\\.\\d+)?/ );\n\n\n\n    $self->register_result(\n\n\n        expected => 1.21,\n\n\n        got      => $gigawatts,\n\n\n        cmp      => '\n>=\n',\n\n\n        hint     => '\nWe\n \nrequire\n \nat\n \nleast\n \n1.21\n \ngigawatts\n \nof\n \npower\n \nfor\n \nthis\n \ndevice\n'\n\n    \n);\n\n\n}\n\n\n\n1\n;\n\n\n\n\n\nValidating input\n\u00b6\n\n\nAs you might notice in the example, it can be quite tedious to validate every\nfield in the input data hash and call \n$self->die\n if it's not present.\nInstead, you may optionally define a 'schema' field. If \nschema\n is defined\nwill validate the input data against the schema before calling \nvalidate\n if\nthe data is valid. Any schema errors will be registered as validation results\nwith 'error' status. \n\n\nThe schema is defined using \nJSON-Schema\n, written with\na Perl hash instead of a JSON string. You can find \nan easy JSON-Schema\ntutorial here\n. It covers everything you\nshould need to write a schema for a Conch Validation. \n\n\nNOTE:\n As the input data is required to be a hash, the root-level \n{ type =>\n'object', properties => {...} }\n is omitted when defining validation schemas.\nAll top-level keys in the \nschema\n hash are assumed to be properties of the\nhash. All top-level keys are also marked as \nrequired\n.\n\n\nWe can re-write our example validation using a schema like so:\n\n\npackage\n \nConch::Validation::MyValidation\n;\n\n\n\nuse\n \nMojo::Base\n \n'Conch::Validation'\n;\n\n\n\nhas\n \n'name'\n \n=>\n \n'my_validation_name'\n;\n\n\nhas\n \n'version'\n \n=>\n \n1\n;\n\n\nhas\n \n'category'\n \n=>\n \n'EXAMPLE'\n;\n\n\nhas\n \n'description'\n \n=>\n \nq(A basic example of a validation.)\n;\n\n\n\n# NOTICE the 'sub {}' wrapping the hash!\n\n\n# Mojo::Base requires non-scalar fields be defined this way.\n\n\nhas\n \n'schema'\n \n=>\n \nsub\n \n{\n\n    \n{\n\n        \npower\n \n=>\n \n{\n\n            \ntype\n       \n=>\n \n'object'\n,\n\n            \nrequired\n   \n=>\n \n[\n'gigawatts'\n],\n\n            \nproperties\n \n=>\n \n{\n\n                \ngigawatts\n \n=>\n \n{\n \ntype\n \n=>\n \n'number'\n \n}\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n};\n\n\n\nsub\n \nvalidate\n \n{\n\n    \nmy\n \n(\n$self\n,\n \n$data\n)\n \n=\n \n@_\n;\n\n    \n# we can safely assume this hash path is present because it was verified with the schema\n\n    \nmy\n \n$gigawatts\n \n=\n \n$data\n->\n{\npower\n}\n->\n{\ngigawatts\n};\n\n\n    \n$self\n->\nregister_result\n(\n\n        \nexpected\n \n=>\n \n1.21\n,\n\n        \ngot\n      \n=>\n \n$gigawatts\n,\n\n        \ncmp\n      \n=>\n \n'>='\n,\n\n        \nhint\n     \n=>\n \n'We require at least 1.21 gigawatts of power for this device'\n\n    \n);\n\n\n}\n\n\n\n1\n;\n\n\n\n\n\nIf the input data does not satisfy the schema, a validation result with status\n'error' will be recorded describing all schema errors and the \nvalidate\n method\nwill not be executed.\n\n\nDispatching on Device Attributes\n\u00b6\n\n\nYour validation may need to evaluate differently based on attributes of the\ndevice under validation, such as the device hardware product, device location,\nand device settings. The \n$self\n reference provides methods for accessing\ndetails for the device under validation\n\n\n\n\n$self->device\n: the \nConch::Model::Device\n object representing the device\n  under validation\n\n\n$self->device_settings\n: a hash ref of the device settings currently stored\n  for the device under validation\n\n\n$self->device_location\n: the \nConch::Class::DeviceLocation\n object\n  representing the location of the device under validation\n\n\n$self->hardware_product_vendor\n: a string of hardware product vendor name of\n  the expected hardware product for the device\n\n\n$self->hardware_product_name\n: a string of the hardware product name of the\n  expected hardware product for the device\n\n\n$self->hardware_product_profile\n: the \nConch::Class:HardareProductProfile\n\n  object representing the hardware product profile of the expected hardware\n  product for the device\n\n\n\n\nIf these methods are used and the device \ndoes not\n have details (for example,\nif a device does not have a location assigned), the method will call\n\n$self->die\n with a description of the condition that caused it to fail. This\nis good! You can program conditions expecting a device to have a location, and\nit will fail automatically if the device isn't assigned yet. No need to write\nyour own conditional checking logic.\n\n\nA common validation use-case is comparing values reported, such as the amount\nof RAM, to the value expected by the hardware product profile. For our\nvalidation, let's assume that we require 1.21 gigawatts per PSU (stay with me\nhere), where the number of PSUs is specified by the \npsu_total\n field of the\nhardware product profile. To add further complexity, this is only applied for\ndevices with the product name 'DMC-12'. For all other produce names, we require\nthe normal 1.21 gigawatts. Let's update our validation with this more complicated\nlogic:\n\n\npackage\n \nConch::Validation::MyValidation\n;\n\n\n\nuse\n \nMojo::Base\n \n'Conch::Validation'\n;\n\n\n\nhas\n \n'name'\n \n=>\n \n'my_validation_name'\n;\n\n\nhas\n \n'version'\n \n=>\n \n1\n;\n\n\nhas\n \n'category'\n \n=>\n \n'EXAMPLE'\n;\n\n\nhas\n \n'description'\n \n=>\n \nq(A basic example of a validation.)\n;\n\n\n\n# NOTICE the 'sub {}' wrapping the hash!\n\n\n# Mojo::Base requires non-scalar fields be defined this way.\n\n\nhas\n \n'schema'\n \n=>\n \nsub\n \n{\n\n    \n{\n\n        \npower\n \n=>\n \n{\n\n            \ntype\n     \n=>\n \n'object'\n,\n\n            \nrequired\n \n=>\n \n[\n'gigawatts'\n],\n\n            \nproperties\n \n=>\n \n{\n\n                \ngigawatts\n \n=>\n \n{\n \ntype\n \n=>\n \n'number'\n \n}\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n};\n\n\n\nsub\n \nvalidate\n \n{\n\n    \nmy\n \n(\n$self\n,\n \n$data\n)\n \n=\n \n@_\n;\n\n    \n# we can safely assume this hash path is present because it was verified with the schema\n\n    \nmy\n \n$gigawatts\n \n=\n \n$data\n->\n{\npower\n}\n->\n{\ngigawatts\n};\n\n\n    \nmy\n \n$expected_gigawatts\n;\n\n    \nif\n \n(\n$self\n->\nhardware_product_name\n \neq\n \n'DMC-12'\n)\n \n{\n\n        \n$expected_gigawatts\n \n=\n \n1.21\n \n*\n \n$self\n->\nhardware_product_profile\n->\npsu_total\n;\n\n    \n}\n \nelse\n \n{\n\n        \n$expected_gigawatts\n \n=\n \n1.21\n;\n\n    \n}\n\n    \n$self\n->\nregister_result\n(\n\n        \nexpected\n \n=>\n \n$expected_gigawatts\n,\n\n        \ngot\n      \n=>\n \n$gigawatts\n,\n\n        \ncmp\n      \n=>\n \n'>='\n,\n\n        \nhint\n     \n=>\n \n\"We require at least $expected_gigawatts gigawatts for this device\"\n\n    \n);\n\n\n}\n\n\n\n1\n;\n\n\n\n\n\nWriting unit tests for Validations\n\u00b6\n\n\nWriting unit tests for your new validation is strongly encouraged. A test\nharness is provided to make this process easy and practical. At minimum, you\nspecify your validation module name and an array of tests cases to execute with\nyour validation.\n\n\nA test cases is simply a hash with fields describing the input and expected\nresults. Each test case must specify a \ndata\n field for the input data, which\nmust be a hash ref. The test case should specify a \ndescription\n string field\nto help with documenting the test case and debugging test failures. A test case\nshould then define one or many of the following:\n\n\n\n\ndies\n: If set to 1, we expect the validation to call \n$self->die\n based on\n  the input. Defaults to 0.\n\n\nsuccess_num\n: The number of results with status 'pass' to be registered\n  based on the input. Defaults to 0.\n\n\nfailure_num\n: The number of results with status 'fail' to be registered\n  based on the input. Defaults to 0.\n\n\n\n\nAn example to test the validation we've written is provided below. To test this\nyourself, write to the file \nt/validation/my_validation_v1.t\n and run \ncarton\nexec prove t/validation/my_validation_v1.t\n.\n\n\nuse\n \nTest::More\n;\n\n\nuse\n \nTest::Conch::Validation\n;\n\n\n\ntest_validation\n(\n\n    \n'Conch::Validation::MyValidation'\n,\n\n    \nhardware_product\n \n=>\n \n{\n \nname\n \n=>\n \n'Test Product'\n \n},\n\n    \ncases\n \n=>\n \n[\n\n        \n{\n\n            \ndescription\n \n=>\n \n'Providing no input data dies'\n,\n\n            \ndata\n        \n=>\n \n{},\n\n            \ndies\n        \n=>\n \n1\n\n        \n},\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\npower\n' is not a hash, it dies'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n'bad power'\n},\n\n            \ndies\n        \n=>\n \n1\n\n        \n},\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\ngigawatts\n' is not a number, it dies'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n{\n \ngigawatts\n \n=>\n \n'bad'\n \n}\n \n},\n\n            \ndies\n        \n=>\n \n1\n\n        \n},\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\ngigawatts\n' is not at least 1.21, it fails'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n{\n \ngigawatts\n \n=>\n \n1.00\n \n}\n \n},\n\n            \nfailure_num\n \n=>\n \n1\n\n        \n},\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\ngigawatts\n' is exactly least 1.21, it passes'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n{\n \ngigawatts\n \n=>\n \n1.21\n \n}\n \n},\n\n            \nsuccess_num\n \n=>\n \n1\n\n        \n},\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\ngigawatts\n' is more than 1.21, it passes'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n{\n \ngigawatts\n \n=>\n \n1.22\n \n}\n \n},\n\n            \nsuccess_num\n \n=>\n \n1\n\n        \n},\n\n    \n]\n\n\n);\n\n\n\ndone_testing\n();\n\n\n\n\n\nThis tests the basic logic, but doesn't test the edge case we introduced with\n1.21 gigawatts multiplied by the number of PSUs for devices with the hardware\nproduct name 'DMC-12'. The validation harness also allows us to provide fake\nobjects, like hardware product and hardware profile, to be used in the\nvalidation under test. This is done by defining named arguments in the\n\ntest_validation\n function like \nhardware_product\n. \nThe list of available\nnamed arguments and an example are given in the documentation for the test\nharness\n.\n\n\nIn the same file, after the \ntest_validation()\n call and before\n\ndone_testing();\n, we add tests for the edge case:\n\n\ntest_validation\n(\n\n    \n'Conch::Validation::MyValidation'\n,\n\n    \nhardware_product\n \n=>\n \n{\n\n        \nname\n \n=>\n \n'DMC-12'\n,\n\n        \n# profile has 2 PSUs\n\n        \nprofile\n \n=>\n \n{\n \npsu_total\n \n=>\n \n2\n \n},\n\n    \n},\n\n    \ncases\n \n=>\n \n[\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\ngigawatts\n' is 1.21, it fails'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n{\n \ngigawatts\n \n=>\n \n1.21\n \n}\n \n},\n\n            \nfailure_num\n \n=>\n \n1\n\n        \n},\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\ngigawatts\n' is exactly 2 * 1.21 = 2.42, it passes'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n{\n \ngigawatts\n \n=>\n \n2.42\n \n}\n \n},\n\n            \nsuccess_num\n \n=>\n \n1\n\n        \n},\n\n        \n{\n\n            \ndescription\n \n=>\n \n'If '\ngigawatts\n' is more than 2.42, it passes'\n,\n\n            \ndata\n        \n=>\n \n{\n \npower\n \n=>\n \n{\n \ngigawatts\n \n=>\n \n2.5\n \n}\n \n},\n\n            \nsuccess_num\n \n=>\n \n1\n\n        \n},\n\n    \n]\n\n\n);\n\n\n\n\n\nDeploying the Validation\n\u00b6\n\n\nA Conch Validation must be deployed in the production Conch instance to be\navailable. After you've written and tested your validation, commit it on a\nbranch and \ncreate a pull request on the Conch Github\nrepository\n. Request someone from the\nConch team to review and merge it.\n\n\nThe validation will be deployed in the next \nversioned\nrelease\n after it has been merged\ninto master.\n\n\nNOTE\n: As of 2018-04-18, device reports are validated during ingest against\none of two validation plans, named \nConch v1 Legacy Plan: Switch\n and \nConch v1\nLegacy Plan: Server\n. \nThe validations associated with these plans are configured\nin the Conch configuration file\n.\nThese validations plans may be modified via the API, but the plans will be\nreset to the specified configuration every time the Conch service is restarted.\nTo include your new validation permanently as part of one of these plans, the\nconfiguration file must be updated in configuration management and deployed.\n\n\nUsing Validations with the Conch Shell\n\u00b6\n\n\nThe \nConch Shell\n CLI tool provides\ncommands for managing validations and validation plans. \nYou may download the\nlatest version of a compiled binary for your system\nhere.\n. Save the binary\nsomewhere on your $PATH. To set up and authenticate your account, run the\ncommand \nconch profile create\n once installed.\n\n\nListing validations\n\u00b6\n\n\nconch validations\n lists all available validations.  Each validation has an\nUUID ID. In commands on a single validation, you may use the full idea or the\nfirst 8 hexadecimal digits of the UUID (all characters before the first dash).\nFor example, \n39cb3ab6-1963-4c9a-94ea-e2d9258d8be0\n may be shortened to\n\n39cb3ab6\n.\n\n\nTesting a validation with a device\n\u00b6\n\n\nconch validation VALIDATION_ID test DEVICE_ID\n tests a validation against a\ndevice. The \nDEVICE_ID\n is a device's serial number. The command returns a\ntable of the validation results from running the validation. \nThis does not\nstore the validations results in the database\n. This command is intended for\ntesting a new validation or new reporting agent code.\n\n\nconch validation VALIDATION_ID test DEVICE_ID\n receives the input\ndata to validate from STDIN. The input data must be in JSON format. For\nexample, you can do any of the following to test a validation:\n\n\nconch validation 39cb3ab6 \ntest\n COFFEE < report_data.json\n\n\n# 'jo' is https://github.com/jpmens/jo\n\njo \n'power[gigawatts]=1.21'\n  \n|\n conch validation 39cb3ab6 \ntest\n COFFEE\n\nconch validation 39cb3ab6 \ntest\n COFFEE \n<<EOF\n\n\n{\n\n\n    \"power\" : {\n\n\n        \"gigawatts\" : 1.21\n\n\n    }\n\n\n}\n\n\nEOF\n\n\n\n\n\nOptionally, you also start the command (hitting enter after the command) and\ntype in your input data and end with \n^D\n (Control-D)\n\n\nconch validation 39cb3ab6 test COFFEE<ENTER>\n{\n    \"power\" : {\n        \"gigawatts\" : 1.21\n    }\n}\n^D\n\n\n\n\nCreating and managing validation plans\n\u00b6\n\n\nValidation plans are collections of validations. Validations plans are is\nexecuted during device report ingest and by orchestration workflows.\nValidations are independent and un-ordered within a validation plan. A given\nvalidation may be in 0 or many validation plans, and a validation plan may have\n0, 1, or many validations associated with it.\n\n\nOnly users who are global admins (i.e., \nthey have an \nAdministrator\n role for\nthe GLOBAL workspace\n may\ncreate and manage validation plans. However, anyone with a Conch account may\nlist and test validation plans.\n\n\nconch validation-plans get\n lists all available validation plans. Like\nvalidation IDs, you may shorten the UUID to the first 8 characters in commands.\n\n\nconch validation-plans create --name NAME_OF_NEW_VALIDATION_PLAN --description\nDESCRIPTION_OF_PLAN\n creates a new validation plan. A new validation plan will\nhave no validations associated with it.\n\n\nconch validation-plan PLAN_ID validations\n lists all validations associated\nwith the plan.\n\n\nconch validation-plan PLAN_ID add-validation VALIDATION_ID\n associates a\nvalidation with the validation plan. You may use a short ID for the\n\nVALIDATION_ID\n field.\n\n\nconch validation-plan PLAN_ID remove-validation VALIDATION_ID\n removes an\nassociated validation from the validation plan. You may use a short ID for the\n\nVALIDATION_ID\n field.\n\n\nTesting validation plans\n\u00b6\n\n\nconch validation-plan VALIDATION_PLAN_ID test DEVICE_ID\n tests a validation\nplan against a device. Any authenticated user may test a validation plan\nagainst a device. Testing with a validation plan works identically to testing\nwith a single validation, except the input data is processed by all validations\nin the validation plan.  This command is useful for verifying a device report\ncan satisfy the schemas for all validations in a validation plans when\ndeveloping a reporting agent.\n\n\nLike testing a validation, the command receives the JSON-formatted input data\nfrom STDIN. Any of the following options work.\n\n\nconch validation-plan 39cb3ab6 \ntest\n COFFEE < report_data.json\n\n\n# 'jo' is https://github.com/jpmens/jo\n\njo \n'power[gigawatts]=1.21'\n \n|\n conch validation-plan 39cb3ab6 \ntest\n COFFEE\n\nconch validation-plan 39cb3ab6 \ntest\n COFFEE \n<<EOF\n\n\n{\n\n\n    \"power\" : {\n\n\n        \"gigawatts\" : 1.21\n\n\n    }\n\n\n}\n\n\nEOF\n\n\nconch validation 39cb3ab6 \ntest\n COFFEE<ENTER>\n\n{\n\n    \n\"power\"\n : \n{\n\n        \n\"gigawatts\"\n : \n1\n.21\n    \n}\n\n\n}\n\n^D\n\n\n\n\nTips for Writing Validations\n\u00b6\n\n\nBelow are some tips for writing effective validations.\n\n\nProvide user-friendly hints\n\u00b6\n\n\n$self->die\n, \n$self->fail\n, and \n$self->register_result\n can be called with an\n\nhint\n attribute to provide a user-friendly message to help diagnose and fix\nthe issue. For example:\n\n\n$self\n->\nregister_result\n(\n\n    \nexpected\n \n=>\n \n$expected_temp\n,\n\n    \ngot\n      \n=>\n \n$temp\n,\n\n    \ncmp\n      \n=>\n \n'<'\n,\n\n    \nhint\n     \n=>\n \n\"This device is too hot! Make it cooler!\"\n\n\n);\n\n\n\n\n\nDie fast and loud\n\u00b6\n\n\nIf there's some condition you don't expect, use \n$self->die\n. Don't try to\nwrite complicated logic to handle all possible conditions. This leads into the\nnext tip.\n\n\nWrite \nsmall\n validations\n\u00b6\n\n\nDon't try to write a validation for all possible hardware products. Validations\ncan be mixed-and-matched in validation plans, and those plans associated with\ndifferent hardware products.  For example, instead of writing conditional logic\nto handle the case for the \"DMC-12\" product, we could have written two separate\nvalidations: one for products where we expect 1.21 * $number_of_PSUs gigawatts\nand one where 1.21 gigawatts is always expected.\n\n\nThe advantage is we don't have to change the validation if there's a new\nhardware product where same edge case should apply. We only need to associate\nthe validation with a validation plan the hardware product should use.",
            "title": "Writing Validations"
        },
        {
            "location": "/development/validations/#writing-deploying-and-testing-a-conch-validation",
            "text": "",
            "title": "Writing, Deploying, and Testing a Conch Validation"
        },
        {
            "location": "/development/validations/#overview-of-steps",
            "text": "Create a sub-class of  Conch::Validation , the Validation base class. Set\n   the  name ,  version ,  description , and  category  fields, and write a\n    validate  method. The  validate  method body should register one or many\n   results or an error.  Create a test file in  t/validation . Use the  Test::Conch::Validation  test\n   harness to write a test cases for your new validation. Verify it works as\n   expected, especially if there are edge cases in the logic.  Commit the validation and validation test on a branch.  Create a pull\n   request in the Conch repository .\n   Request a review.  Once your pull request has been reviewed, merged into master, and deployed,\n   the validation is available on the system and via the API.  Using the  Conch Shell CLI tool , you\n   can do the following:  List available validations.  conch validations  Test a validation against a device using supplied data.  conch\n   validation VALIDATION_ID test DEVICE_ID  Add a validation to a validation plan.  conch validation-plan\n   VALIDATION_PLAN add-validation VALIDATION_ID     Documentation for the  Validation base class \nand  Validation test harness \nare useful references for writing and testing a new Conch Validation.",
            "title": "Overview of steps"
        },
        {
            "location": "/development/validations/#setting-up-the-conch-repository",
            "text": "All Conch Validations are code modules committed in the  Conch\nrepository . To add new validations, you must\ncreate a local clone of the repository and add new validations as files in the\nrepository.  Clone the repository locally and set it's path to your working directory. All\nfile paths in this tutorial are relative to the root of the repository.  To build and run Conch locally, Perl version 5.26.0, Carton , and yarn  are all required to be installed on your system.  To build conch, run  make build  in  Conch/ .  make test  runs all tests,\nincluding validation tests. To run the Conch server, run  make run  in Conch/ . All valid Conch Validations are automatically loaded when the Conch\nserver is started.",
            "title": "Setting up the Conch Repository"
        },
        {
            "location": "/development/validations/#creating-a-new-validation",
            "text": "Let's start by creating a simple yet valid validation. Create and write the\nfollowing code to the file  Conch/lib/Conch/Validation/MyValidation.pm :  package   Conch::Validation::MyValidation ;  use   Mojo::Base   'Conch::Validation' ;  has   'name'   =>   'my_validation_name' ;  has   'version'   =>   1 ;  has   'category'   =>   'EXAMPLE' ;  has   'description'   =>   q(A basic example of a validation.) ;  sub   validate   {}  1 ;   This validation doesn't do anything useful and will not record results when\nexecuted. But it satisfies all of the basic requirements to be loaded as a new\nValidation into Conch. You could run  make run  in  Conch/  and this validation\nwill be loaded into the system.  Valid validations must be sub-classes of the  Conch::Validation  module, and\nthe package must be under the  Conch::Validation  namespace. Validations use\nthe  Mojo::Base  module for sub-classing\nand to set fields with the  has  directive. The following fields are required\nfor every validation:   name : a short string name of the validation. The  name  and  version \n  values together must be unique for all validations in the system.  version : an integer denoting the version of the validation for a given\n  name. This field allows for supporting multiple versions of similar\n  validations for legacy support.  category : a short string signifier denoting the device component category\n  being validated. Example categories are  BIOS ,  NET ,  DISK ,  CPU ,\n   RAM . Upper case is used by convention.  description : a longer, human-friendly description about the validation. The\n  description should describe what is being validated and how.   All validations must define the  validate  method.  validate  defines the logic\nof the validation. In our example, it does nothing, but next we will define\nvalidation logic and register results.",
            "title": "Creating a new Validation"
        },
        {
            "location": "/development/validations/#writing-validation-logic",
            "text": "The  validate  method defines the validation logic. When a validation is run in\nthe validation system, the  validate  method is called for each validation.  The  validate  method receives two arguments: a 'self' reference and a hash ref\nof the data input to the validation. Like so:  sub   validate   { \n     my   ( $self ,   $data )   =   @_ ; \n     # assuming the input data has a 'product_name' field \n     my   $product_name   =   $data -> { product_name };  }   The validation logic should register at least one validation result and can\nregister as many results as desired. Validation results can be registered with\nthe following methods with different effects:    $self->die('string') :\n  Stop execution of the validation and record a validation result with status\n  'error'. This should be used when the validation cannot continue. For example,\n  when expected values in the  $data  hash ref are not present, call\n   $self->die()  with a description of the expected value    $self->fail('string') :\n  Record a validation result with status 'fail' and continue execution of the\n  validation. This may be used if some precondition is not satisfied but the\n  validation should still continue.    $self->register_result( expected => $a, got => $b, cmp => 'eq') :\n  this is the workhorse of validation logic. It takes an expected and 'got' value\n  and compares them with the operator  cmp . The list of available comparison\n  operators can be  found in the documentation.    For our example, let's validate that the input data reports that the device has\nat least 1.21 gigawatts of power. This would be reported in JSON format and\nde-serialized to the Perl hash ref provided as the  $data  argument. The input\ndata should look like  { \"power\": { \"gigawatts\" : <number> } } . We must be\nsure to check we have received the data in the format we expect so our\nvalidation is verifying the expected values. Update your validation file to the\nfollowing:  package   Conch::Validation::MyValidation ;  use   Mojo::Base   'Conch::Validation' ;  has   'name'   =>   'my_validation_name' ;  has   'version'   =>   1 ;  has   'category'   =>   'EXAMPLE' ;  has   'description'   =>   q(A basic example of a validation.) ;  sub   validate   { \n     my   ( $self ,   $data )   =   @_ ; \n     my   $power   =   $data -> { power }; \n\n     $self -> die ( \"'power' key is required in the input data\" )   unless   defined ( $power ); \n     $self -> die ( \"'power' value must be a hash\" )   unless   ref ( $power )   eq   'HASH' ; \n\n     my   $gigawatts   =   $power -> { gigawatts }; \n\n     $self -> die ( \"'gigawatts' is required in the input data\" )   unless   defined ( $gigawatts ); \n     $self -> die ( \" 'gigawatts'   must   be   a   number ') unless ( $gigawatts =~ /\\d+(\\.\\d+)?/ );      $self->register_result(          expected => 1.21,          got      => $gigawatts,          cmp      => ' >= ',          hint     => ' We   require   at   least   1.21   gigawatts   of   power   for   this   device ' \n     );  }  1 ;",
            "title": "Writing validation logic"
        },
        {
            "location": "/development/validations/#validating-input",
            "text": "As you might notice in the example, it can be quite tedious to validate every\nfield in the input data hash and call  $self->die  if it's not present.\nInstead, you may optionally define a 'schema' field. If  schema  is defined\nwill validate the input data against the schema before calling  validate  if\nthe data is valid. Any schema errors will be registered as validation results\nwith 'error' status.   The schema is defined using  JSON-Schema , written with\na Perl hash instead of a JSON string. You can find  an easy JSON-Schema\ntutorial here . It covers everything you\nshould need to write a schema for a Conch Validation.   NOTE:  As the input data is required to be a hash, the root-level  { type =>\n'object', properties => {...} }  is omitted when defining validation schemas.\nAll top-level keys in the  schema  hash are assumed to be properties of the\nhash. All top-level keys are also marked as  required .  We can re-write our example validation using a schema like so:  package   Conch::Validation::MyValidation ;  use   Mojo::Base   'Conch::Validation' ;  has   'name'   =>   'my_validation_name' ;  has   'version'   =>   1 ;  has   'category'   =>   'EXAMPLE' ;  has   'description'   =>   q(A basic example of a validation.) ;  # NOTICE the 'sub {}' wrapping the hash!  # Mojo::Base requires non-scalar fields be defined this way.  has   'schema'   =>   sub   { \n     { \n         power   =>   { \n             type         =>   'object' , \n             required     =>   [ 'gigawatts' ], \n             properties   =>   { \n                 gigawatts   =>   {   type   =>   'number'   } \n             } \n         } \n     }  };  sub   validate   { \n     my   ( $self ,   $data )   =   @_ ; \n     # we can safely assume this hash path is present because it was verified with the schema \n     my   $gigawatts   =   $data -> { power } -> { gigawatts }; \n\n     $self -> register_result ( \n         expected   =>   1.21 , \n         got        =>   $gigawatts , \n         cmp        =>   '>=' , \n         hint       =>   'We require at least 1.21 gigawatts of power for this device' \n     );  }  1 ;   If the input data does not satisfy the schema, a validation result with status\n'error' will be recorded describing all schema errors and the  validate  method\nwill not be executed.",
            "title": "Validating input"
        },
        {
            "location": "/development/validations/#dispatching-on-device-attributes",
            "text": "Your validation may need to evaluate differently based on attributes of the\ndevice under validation, such as the device hardware product, device location,\nand device settings. The  $self  reference provides methods for accessing\ndetails for the device under validation   $self->device : the  Conch::Model::Device  object representing the device\n  under validation  $self->device_settings : a hash ref of the device settings currently stored\n  for the device under validation  $self->device_location : the  Conch::Class::DeviceLocation  object\n  representing the location of the device under validation  $self->hardware_product_vendor : a string of hardware product vendor name of\n  the expected hardware product for the device  $self->hardware_product_name : a string of the hardware product name of the\n  expected hardware product for the device  $self->hardware_product_profile : the  Conch::Class:HardareProductProfile \n  object representing the hardware product profile of the expected hardware\n  product for the device   If these methods are used and the device  does not  have details (for example,\nif a device does not have a location assigned), the method will call $self->die  with a description of the condition that caused it to fail. This\nis good! You can program conditions expecting a device to have a location, and\nit will fail automatically if the device isn't assigned yet. No need to write\nyour own conditional checking logic.  A common validation use-case is comparing values reported, such as the amount\nof RAM, to the value expected by the hardware product profile. For our\nvalidation, let's assume that we require 1.21 gigawatts per PSU (stay with me\nhere), where the number of PSUs is specified by the  psu_total  field of the\nhardware product profile. To add further complexity, this is only applied for\ndevices with the product name 'DMC-12'. For all other produce names, we require\nthe normal 1.21 gigawatts. Let's update our validation with this more complicated\nlogic:  package   Conch::Validation::MyValidation ;  use   Mojo::Base   'Conch::Validation' ;  has   'name'   =>   'my_validation_name' ;  has   'version'   =>   1 ;  has   'category'   =>   'EXAMPLE' ;  has   'description'   =>   q(A basic example of a validation.) ;  # NOTICE the 'sub {}' wrapping the hash!  # Mojo::Base requires non-scalar fields be defined this way.  has   'schema'   =>   sub   { \n     { \n         power   =>   { \n             type       =>   'object' , \n             required   =>   [ 'gigawatts' ], \n             properties   =>   { \n                 gigawatts   =>   {   type   =>   'number'   } \n             } \n         } \n     }  };  sub   validate   { \n     my   ( $self ,   $data )   =   @_ ; \n     # we can safely assume this hash path is present because it was verified with the schema \n     my   $gigawatts   =   $data -> { power } -> { gigawatts }; \n\n     my   $expected_gigawatts ; \n     if   ( $self -> hardware_product_name   eq   'DMC-12' )   { \n         $expected_gigawatts   =   1.21   *   $self -> hardware_product_profile -> psu_total ; \n     }   else   { \n         $expected_gigawatts   =   1.21 ; \n     } \n     $self -> register_result ( \n         expected   =>   $expected_gigawatts , \n         got        =>   $gigawatts , \n         cmp        =>   '>=' , \n         hint       =>   \"We require at least $expected_gigawatts gigawatts for this device\" \n     );  }  1 ;",
            "title": "Dispatching on Device Attributes"
        },
        {
            "location": "/development/validations/#writing-unit-tests-for-validations",
            "text": "Writing unit tests for your new validation is strongly encouraged. A test\nharness is provided to make this process easy and practical. At minimum, you\nspecify your validation module name and an array of tests cases to execute with\nyour validation.  A test cases is simply a hash with fields describing the input and expected\nresults. Each test case must specify a  data  field for the input data, which\nmust be a hash ref. The test case should specify a  description  string field\nto help with documenting the test case and debugging test failures. A test case\nshould then define one or many of the following:   dies : If set to 1, we expect the validation to call  $self->die  based on\n  the input. Defaults to 0.  success_num : The number of results with status 'pass' to be registered\n  based on the input. Defaults to 0.  failure_num : The number of results with status 'fail' to be registered\n  based on the input. Defaults to 0.   An example to test the validation we've written is provided below. To test this\nyourself, write to the file  t/validation/my_validation_v1.t  and run  carton\nexec prove t/validation/my_validation_v1.t .  use   Test::More ;  use   Test::Conch::Validation ;  test_validation ( \n     'Conch::Validation::MyValidation' , \n     hardware_product   =>   {   name   =>   'Test Product'   }, \n     cases   =>   [ \n         { \n             description   =>   'Providing no input data dies' , \n             data          =>   {}, \n             dies          =>   1 \n         }, \n         { \n             description   =>   'If ' power ' is not a hash, it dies' , \n             data          =>   {   power   =>   'bad power' }, \n             dies          =>   1 \n         }, \n         { \n             description   =>   'If ' gigawatts ' is not a number, it dies' , \n             data          =>   {   power   =>   {   gigawatts   =>   'bad'   }   }, \n             dies          =>   1 \n         }, \n         { \n             description   =>   'If ' gigawatts ' is not at least 1.21, it fails' , \n             data          =>   {   power   =>   {   gigawatts   =>   1.00   }   }, \n             failure_num   =>   1 \n         }, \n         { \n             description   =>   'If ' gigawatts ' is exactly least 1.21, it passes' , \n             data          =>   {   power   =>   {   gigawatts   =>   1.21   }   }, \n             success_num   =>   1 \n         }, \n         { \n             description   =>   'If ' gigawatts ' is more than 1.21, it passes' , \n             data          =>   {   power   =>   {   gigawatts   =>   1.22   }   }, \n             success_num   =>   1 \n         }, \n     ]  );  done_testing ();   This tests the basic logic, but doesn't test the edge case we introduced with\n1.21 gigawatts multiplied by the number of PSUs for devices with the hardware\nproduct name 'DMC-12'. The validation harness also allows us to provide fake\nobjects, like hardware product and hardware profile, to be used in the\nvalidation under test. This is done by defining named arguments in the test_validation  function like  hardware_product .  The list of available\nnamed arguments and an example are given in the documentation for the test\nharness .  In the same file, after the  test_validation()  call and before done_testing(); , we add tests for the edge case:  test_validation ( \n     'Conch::Validation::MyValidation' , \n     hardware_product   =>   { \n         name   =>   'DMC-12' , \n         # profile has 2 PSUs \n         profile   =>   {   psu_total   =>   2   }, \n     }, \n     cases   =>   [ \n         { \n             description   =>   'If ' gigawatts ' is 1.21, it fails' , \n             data          =>   {   power   =>   {   gigawatts   =>   1.21   }   }, \n             failure_num   =>   1 \n         }, \n         { \n             description   =>   'If ' gigawatts ' is exactly 2 * 1.21 = 2.42, it passes' , \n             data          =>   {   power   =>   {   gigawatts   =>   2.42   }   }, \n             success_num   =>   1 \n         }, \n         { \n             description   =>   'If ' gigawatts ' is more than 2.42, it passes' , \n             data          =>   {   power   =>   {   gigawatts   =>   2.5   }   }, \n             success_num   =>   1 \n         }, \n     ]  );",
            "title": "Writing unit tests for Validations"
        },
        {
            "location": "/development/validations/#deploying-the-validation",
            "text": "A Conch Validation must be deployed in the production Conch instance to be\navailable. After you've written and tested your validation, commit it on a\nbranch and  create a pull request on the Conch Github\nrepository . Request someone from the\nConch team to review and merge it.  The validation will be deployed in the next  versioned\nrelease  after it has been merged\ninto master.  NOTE : As of 2018-04-18, device reports are validated during ingest against\none of two validation plans, named  Conch v1 Legacy Plan: Switch  and  Conch v1\nLegacy Plan: Server .  The validations associated with these plans are configured\nin the Conch configuration file .\nThese validations plans may be modified via the API, but the plans will be\nreset to the specified configuration every time the Conch service is restarted.\nTo include your new validation permanently as part of one of these plans, the\nconfiguration file must be updated in configuration management and deployed.",
            "title": "Deploying the Validation"
        },
        {
            "location": "/development/validations/#using-validations-with-the-conch-shell",
            "text": "The  Conch Shell  CLI tool provides\ncommands for managing validations and validation plans.  You may download the\nlatest version of a compiled binary for your system\nhere. . Save the binary\nsomewhere on your $PATH. To set up and authenticate your account, run the\ncommand  conch profile create  once installed.",
            "title": "Using Validations with the Conch Shell"
        },
        {
            "location": "/development/validations/#listing-validations",
            "text": "conch validations  lists all available validations.  Each validation has an\nUUID ID. In commands on a single validation, you may use the full idea or the\nfirst 8 hexadecimal digits of the UUID (all characters before the first dash).\nFor example,  39cb3ab6-1963-4c9a-94ea-e2d9258d8be0  may be shortened to 39cb3ab6 .",
            "title": "Listing validations"
        },
        {
            "location": "/development/validations/#testing-a-validation-with-a-device",
            "text": "conch validation VALIDATION_ID test DEVICE_ID  tests a validation against a\ndevice. The  DEVICE_ID  is a device's serial number. The command returns a\ntable of the validation results from running the validation.  This does not\nstore the validations results in the database . This command is intended for\ntesting a new validation or new reporting agent code.  conch validation VALIDATION_ID test DEVICE_ID  receives the input\ndata to validate from STDIN. The input data must be in JSON format. For\nexample, you can do any of the following to test a validation:  conch validation 39cb3ab6  test  COFFEE < report_data.json # 'jo' is https://github.com/jpmens/jo \njo  'power[gigawatts]=1.21'    |  conch validation 39cb3ab6  test  COFFEE\n\nconch validation 39cb3ab6  test  COFFEE  <<EOF  {      \"power\" : {          \"gigawatts\" : 1.21      }  }  EOF   Optionally, you also start the command (hitting enter after the command) and\ntype in your input data and end with  ^D  (Control-D)  conch validation 39cb3ab6 test COFFEE<ENTER>\n{\n    \"power\" : {\n        \"gigawatts\" : 1.21\n    }\n}\n^D",
            "title": "Testing a validation with a device"
        },
        {
            "location": "/development/validations/#creating-and-managing-validation-plans",
            "text": "Validation plans are collections of validations. Validations plans are is\nexecuted during device report ingest and by orchestration workflows.\nValidations are independent and un-ordered within a validation plan. A given\nvalidation may be in 0 or many validation plans, and a validation plan may have\n0, 1, or many validations associated with it.  Only users who are global admins (i.e.,  they have an  Administrator  role for\nthe GLOBAL workspace  may\ncreate and manage validation plans. However, anyone with a Conch account may\nlist and test validation plans.  conch validation-plans get  lists all available validation plans. Like\nvalidation IDs, you may shorten the UUID to the first 8 characters in commands.  conch validation-plans create --name NAME_OF_NEW_VALIDATION_PLAN --description\nDESCRIPTION_OF_PLAN  creates a new validation plan. A new validation plan will\nhave no validations associated with it.  conch validation-plan PLAN_ID validations  lists all validations associated\nwith the plan.  conch validation-plan PLAN_ID add-validation VALIDATION_ID  associates a\nvalidation with the validation plan. You may use a short ID for the VALIDATION_ID  field.  conch validation-plan PLAN_ID remove-validation VALIDATION_ID  removes an\nassociated validation from the validation plan. You may use a short ID for the VALIDATION_ID  field.",
            "title": "Creating and managing validation plans"
        },
        {
            "location": "/development/validations/#testing-validation-plans",
            "text": "conch validation-plan VALIDATION_PLAN_ID test DEVICE_ID  tests a validation\nplan against a device. Any authenticated user may test a validation plan\nagainst a device. Testing with a validation plan works identically to testing\nwith a single validation, except the input data is processed by all validations\nin the validation plan.  This command is useful for verifying a device report\ncan satisfy the schemas for all validations in a validation plans when\ndeveloping a reporting agent.  Like testing a validation, the command receives the JSON-formatted input data\nfrom STDIN. Any of the following options work.  conch validation-plan 39cb3ab6  test  COFFEE < report_data.json # 'jo' is https://github.com/jpmens/jo \njo  'power[gigawatts]=1.21'   |  conch validation-plan 39cb3ab6  test  COFFEE\n\nconch validation-plan 39cb3ab6  test  COFFEE  <<EOF  {      \"power\" : {          \"gigawatts\" : 1.21      }  }  EOF \n\nconch validation 39cb3ab6  test  COFFEE<ENTER> { \n     \"power\"  :  { \n         \"gigawatts\"  :  1 .21\n     }  } \n^D",
            "title": "Testing validation plans"
        },
        {
            "location": "/development/validations/#tips-for-writing-validations",
            "text": "Below are some tips for writing effective validations.",
            "title": "Tips for Writing Validations"
        },
        {
            "location": "/development/validations/#provide-user-friendly-hints",
            "text": "$self->die ,  $self->fail , and  $self->register_result  can be called with an hint  attribute to provide a user-friendly message to help diagnose and fix\nthe issue. For example:  $self -> register_result ( \n     expected   =>   $expected_temp , \n     got        =>   $temp , \n     cmp        =>   '<' , \n     hint       =>   \"This device is too hot! Make it cooler!\"  );",
            "title": "Provide user-friendly hints"
        },
        {
            "location": "/development/validations/#die-fast-and-loud",
            "text": "If there's some condition you don't expect, use  $self->die . Don't try to\nwrite complicated logic to handle all possible conditions. This leads into the\nnext tip.",
            "title": "Die fast and loud"
        },
        {
            "location": "/development/validations/#write-small-validations",
            "text": "Don't try to write a validation for all possible hardware products. Validations\ncan be mixed-and-matched in validation plans, and those plans associated with\ndifferent hardware products.  For example, instead of writing conditional logic\nto handle the case for the \"DMC-12\" product, we could have written two separate\nvalidations: one for products where we expect 1.21 * $number_of_PSUs gigawatts\nand one where 1.21 gigawatts is always expected.  The advantage is we don't have to change the validation if there's a new\nhardware product where same edge case should apply. We only need to associate\nthe validation with a validation plan the hardware product should use.",
            "title": "Write small validations"
        }
    ]
}